# Text-Generation using with Python and TensorFlow/Keras

(Text Generation using a Recurrent Neural Networks, specifically a Long Short-Term Memory Network, implementing this network in Python, and use it to generate some text.)

To begin with, let's start by defining our terms:-

**TensorFlow**: TensorFlow is one of the most commonly used machine learning libraries in Python, specializing in the creation of deep neural networks. Deep neural networks excel at tasks like image recognition and recognizing patterns in speech. TensorFlow was designed by Google Brain, and its power lies in its ability to join together many different processing nodes.

**Keras**: Keras is an application programming interface or API. Keras makes use of TensorFlow's functions and abilities, but it streamlines the implementation of TensorFlow functions, making building a neural network much simpler and easier. Keras' foundational principles are modularity and user-friendliness, meaning that while Keras is quite powerful, it is easy to use and scale.

**Natural Language Processing**: Natural Language Processing (NLP) is, the techniques used to enable computers to understand natural human language, rather than having to interface with people through programming languages. Natural language processing is necessary for tasks like the classification of word documents or the creation of a chatbot.

**Corpus**: A Corpus is a large collection of text, and in the machine learning sense a corpus can be thought of as your model's input data. The corpus contains the text you want the model to learn about. The corpus typically requires preprocessing to become fit for usage in a machine learning system.

**Encoding**: Encoding is sometimes referred to as word representation and it refers to the process of converting text data into a form that a machine learning model can understand. Neural networks cannot work with raw text data, the characters/words must be transformed into a series of numbers the network can interpret.

**Recurrent Neural Network**: Recurrent Neural Networks are useful for text processing because of their ability to remember the different parts of a series of inputs, which means that they can take the previous parts of a sentence into account to interpret context.

**Long Short-Term Memory**: Long Short-Term Memory (LSTMs) networks are a specific type of Recurrent Neural Networks. An LSTM can selectively "forget" information deemed nonessential to the task at hand. By suppressing nonessential information, the LSTM is able to focus on only the information that genuinely matters, taking care of the vanishing gradient problem. This makes LSTMs more robust when handling long strings of text.

