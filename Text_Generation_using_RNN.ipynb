{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation using RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzz2ONW08Cg+U2y2tUS/sj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aashish1106/Text-Generation/blob/main/Text_Generation_using_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg0bhBp7azr4"
      },
      "source": [
        "**Text Generation** using a **Recurrent Neural Networks**, specifically a **Long Short-Term Memory Network**, implementing this network in Python, and use it to generate some text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Opra2UQKTnRA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bef965c0-893d-4cd9-accc-65797eda6a3b"
      },
      "source": [
        "#importing dependencies \n",
        "import numpy\n",
        "import sys\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIUf206xTnYb"
      },
      "source": [
        "#loading the dataset\n",
        "file = open(\"New.txt\").read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1THFMhZnTnd_"
      },
      "source": [
        "#now tokenizing words \n",
        "def tokenize_words(input):\n",
        "    # lowercase everything to standardize it\n",
        "    input = input.lower()\n",
        "\n",
        "    # instantiate the tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(input)\n",
        "\n",
        "    # if the created token isn't in the stop words, make it part of \"filtered\"\n",
        "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
        "    return \" \".join(filtered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSQEIQMsIXHL"
      },
      "source": [
        "# preprocess the input data, makes tokens\n",
        "processed_inputs = tokenize_words(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z56LmkfIXfT"
      },
      "source": [
        "#converting characters to numbers as neural networks works on numbers \n",
        "chars = sorted(list(set(processed_inputs)))\n",
        "char_to_num = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwT220aUIXls",
        "outputId": "dd9d54b9-aafd-480f-8889-5379ec4d73b4"
      },
      "source": [
        "#checking number of characters and vocabularies\n",
        "input_len = len(processed_inputs)\n",
        "vocab_len = len(chars)\n",
        "print (\"Total number of characters:\", input_len)\n",
        "print (\"Total vocab:\", vocab_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 2376\n",
            "Total vocab: 53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxYVlNdlIXqE"
      },
      "source": [
        "seq_length = 100\n",
        "x_data = []\n",
        "y_data = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgtxAmSqIXuM"
      },
      "source": [
        "# loop through inputs, start at the beginning and go until we hit\n",
        "# the final character we can create a sequence out of\n",
        "for i in range(0, input_len - seq_length, 1):\n",
        "    # Define input and output sequences\n",
        "    # Input is the current character plus desired sequence length\n",
        "    in_seq = processed_inputs[i:i + seq_length]\n",
        "\n",
        "    # Out sequence is the initial character plus total sequence length\n",
        "    out_seq = processed_inputs[i + seq_length]\n",
        "\n",
        "    # We now convert list of characters to integers based on\n",
        "    # previously and add the values to our lists\n",
        "    x_data.append([char_to_num[char] for char in in_seq])\n",
        "    y_data.append(char_to_num[out_seq])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFlxDwSWPElh",
        "outputId": "41f02c42-5b71-4274-8f45-9cfcd6d4f1fe"
      },
      "source": [
        "n_patterns = len(x_data)\n",
        "print (\"Total Patterns:\", n_patterns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns: 2276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P5tcT7bPJn3"
      },
      "source": [
        "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
        "X = X/float(vocab_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3wBB0f1PJwm"
      },
      "source": [
        "y = np_utils.to_categorical(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nebCgEvIPOyf"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4LG0YoPPO2H"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JfR5gYiPWPl"
      },
      "source": [
        "filepath = \"model_weights_saved.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "desired_callbacks = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T696XWPUPZgt",
        "outputId": "4b9200d2-9e4c-4750-a295-70da887f7b0c"
      },
      "source": [
        "model.fit(X, y, epochs=160, batch_size=256, callbacks=desired_callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/160\n",
            "9/9 [==============================] - 35s 4s/step - loss: 2.7074\n",
            "\n",
            "Epoch 00001: loss did not improve from 2.69724\n",
            "Epoch 2/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 2.6973\n",
            "\n",
            "Epoch 00002: loss did not improve from 2.69724\n",
            "Epoch 3/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.6755\n",
            "\n",
            "Epoch 00003: loss improved from 2.69724 to 2.67551, saving model to model_weights_saved.hdf5\n",
            "Epoch 4/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 2.6613\n",
            "\n",
            "Epoch 00004: loss improved from 2.67551 to 2.66131, saving model to model_weights_saved.hdf5\n",
            "Epoch 5/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.6463\n",
            "\n",
            "Epoch 00005: loss improved from 2.66131 to 2.64632, saving model to model_weights_saved.hdf5\n",
            "Epoch 6/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 2.6363\n",
            "\n",
            "Epoch 00006: loss improved from 2.64632 to 2.63626, saving model to model_weights_saved.hdf5\n",
            "Epoch 7/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 2.6292\n",
            "\n",
            "Epoch 00007: loss improved from 2.63626 to 2.62916, saving model to model_weights_saved.hdf5\n",
            "Epoch 8/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 2.6267\n",
            "\n",
            "Epoch 00008: loss improved from 2.62916 to 2.62669, saving model to model_weights_saved.hdf5\n",
            "Epoch 9/160\n",
            "9/9 [==============================] - 32s 3s/step - loss: 2.5983\n",
            "\n",
            "Epoch 00009: loss improved from 2.62669 to 2.59826, saving model to model_weights_saved.hdf5\n",
            "Epoch 10/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 2.6024\n",
            "\n",
            "Epoch 00010: loss did not improve from 2.59826\n",
            "Epoch 11/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 2.5936\n",
            "\n",
            "Epoch 00011: loss improved from 2.59826 to 2.59357, saving model to model_weights_saved.hdf5\n",
            "Epoch 12/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 2.5720\n",
            "\n",
            "Epoch 00012: loss improved from 2.59357 to 2.57197, saving model to model_weights_saved.hdf5\n",
            "Epoch 13/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 2.5611\n",
            "\n",
            "Epoch 00013: loss improved from 2.57197 to 2.56114, saving model to model_weights_saved.hdf5\n",
            "Epoch 14/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 2.5604\n",
            "\n",
            "Epoch 00014: loss improved from 2.56114 to 2.56041, saving model to model_weights_saved.hdf5\n",
            "Epoch 15/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.5371\n",
            "\n",
            "Epoch 00015: loss improved from 2.56041 to 2.53708, saving model to model_weights_saved.hdf5\n",
            "Epoch 16/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.5117\n",
            "\n",
            "Epoch 00016: loss improved from 2.53708 to 2.51165, saving model to model_weights_saved.hdf5\n",
            "Epoch 17/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.5016\n",
            "\n",
            "Epoch 00017: loss improved from 2.51165 to 2.50157, saving model to model_weights_saved.hdf5\n",
            "Epoch 18/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.4921\n",
            "\n",
            "Epoch 00018: loss improved from 2.50157 to 2.49208, saving model to model_weights_saved.hdf5\n",
            "Epoch 19/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.4773\n",
            "\n",
            "Epoch 00019: loss improved from 2.49208 to 2.47731, saving model to model_weights_saved.hdf5\n",
            "Epoch 20/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.4506\n",
            "\n",
            "Epoch 00020: loss improved from 2.47731 to 2.45059, saving model to model_weights_saved.hdf5\n",
            "Epoch 21/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.4527\n",
            "\n",
            "Epoch 00021: loss did not improve from 2.45059\n",
            "Epoch 22/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.4255\n",
            "\n",
            "Epoch 00022: loss improved from 2.45059 to 2.42548, saving model to model_weights_saved.hdf5\n",
            "Epoch 23/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.4063\n",
            "\n",
            "Epoch 00023: loss improved from 2.42548 to 2.40626, saving model to model_weights_saved.hdf5\n",
            "Epoch 24/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 2.3880\n",
            "\n",
            "Epoch 00024: loss improved from 2.40626 to 2.38800, saving model to model_weights_saved.hdf5\n",
            "Epoch 25/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 2.3674\n",
            "\n",
            "Epoch 00025: loss improved from 2.38800 to 2.36737, saving model to model_weights_saved.hdf5\n",
            "Epoch 26/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.3468\n",
            "\n",
            "Epoch 00026: loss improved from 2.36737 to 2.34682, saving model to model_weights_saved.hdf5\n",
            "Epoch 27/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 2.3412\n",
            "\n",
            "Epoch 00027: loss improved from 2.34682 to 2.34117, saving model to model_weights_saved.hdf5\n",
            "Epoch 28/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.3288\n",
            "\n",
            "Epoch 00028: loss improved from 2.34117 to 2.32884, saving model to model_weights_saved.hdf5\n",
            "Epoch 29/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.3074\n",
            "\n",
            "Epoch 00029: loss improved from 2.32884 to 2.30742, saving model to model_weights_saved.hdf5\n",
            "Epoch 30/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.2816\n",
            "\n",
            "Epoch 00030: loss improved from 2.30742 to 2.28164, saving model to model_weights_saved.hdf5\n",
            "Epoch 31/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.2629\n",
            "\n",
            "Epoch 00031: loss improved from 2.28164 to 2.26293, saving model to model_weights_saved.hdf5\n",
            "Epoch 32/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.2503\n",
            "\n",
            "Epoch 00032: loss improved from 2.26293 to 2.25033, saving model to model_weights_saved.hdf5\n",
            "Epoch 33/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.2293\n",
            "\n",
            "Epoch 00033: loss improved from 2.25033 to 2.22926, saving model to model_weights_saved.hdf5\n",
            "Epoch 34/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.2168\n",
            "\n",
            "Epoch 00034: loss improved from 2.22926 to 2.21682, saving model to model_weights_saved.hdf5\n",
            "Epoch 35/160\n",
            "9/9 [==============================] - 33s 4s/step - loss: 2.2016\n",
            "\n",
            "Epoch 00035: loss improved from 2.21682 to 2.20162, saving model to model_weights_saved.hdf5\n",
            "Epoch 36/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.1617\n",
            "\n",
            "Epoch 00036: loss improved from 2.20162 to 2.16171, saving model to model_weights_saved.hdf5\n",
            "Epoch 37/160\n",
            "9/9 [==============================] - 32s 3s/step - loss: 2.1577\n",
            "\n",
            "Epoch 00037: loss improved from 2.16171 to 2.15770, saving model to model_weights_saved.hdf5\n",
            "Epoch 38/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 2.1438\n",
            "\n",
            "Epoch 00038: loss improved from 2.15770 to 2.14378, saving model to model_weights_saved.hdf5\n",
            "Epoch 39/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.1271\n",
            "\n",
            "Epoch 00039: loss improved from 2.14378 to 2.12705, saving model to model_weights_saved.hdf5\n",
            "Epoch 40/160\n",
            "9/9 [==============================] - 35s 4s/step - loss: 2.1356\n",
            "\n",
            "Epoch 00040: loss did not improve from 2.12705\n",
            "Epoch 41/160\n",
            "9/9 [==============================] - 33s 4s/step - loss: 2.1249\n",
            "\n",
            "Epoch 00041: loss improved from 2.12705 to 2.12487, saving model to model_weights_saved.hdf5\n",
            "Epoch 42/160\n",
            "9/9 [==============================] - 33s 4s/step - loss: 2.0809\n",
            "\n",
            "Epoch 00042: loss improved from 2.12487 to 2.08092, saving model to model_weights_saved.hdf5\n",
            "Epoch 43/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.0702\n",
            "\n",
            "Epoch 00043: loss improved from 2.08092 to 2.07017, saving model to model_weights_saved.hdf5\n",
            "Epoch 44/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 2.0448\n",
            "\n",
            "Epoch 00044: loss improved from 2.07017 to 2.04481, saving model to model_weights_saved.hdf5\n",
            "Epoch 45/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 2.0243\n",
            "\n",
            "Epoch 00045: loss improved from 2.04481 to 2.02426, saving model to model_weights_saved.hdf5\n",
            "Epoch 46/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.9990\n",
            "\n",
            "Epoch 00046: loss improved from 2.02426 to 1.99901, saving model to model_weights_saved.hdf5\n",
            "Epoch 47/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.9727\n",
            "\n",
            "Epoch 00047: loss improved from 1.99901 to 1.97269, saving model to model_weights_saved.hdf5\n",
            "Epoch 48/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.9563\n",
            "\n",
            "Epoch 00048: loss improved from 1.97269 to 1.95630, saving model to model_weights_saved.hdf5\n",
            "Epoch 49/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.9375\n",
            "\n",
            "Epoch 00049: loss improved from 1.95630 to 1.93754, saving model to model_weights_saved.hdf5\n",
            "Epoch 50/160\n",
            "9/9 [==============================] - 33s 4s/step - loss: 1.9204\n",
            "\n",
            "Epoch 00050: loss improved from 1.93754 to 1.92043, saving model to model_weights_saved.hdf5\n",
            "Epoch 51/160\n",
            "9/9 [==============================] - 33s 4s/step - loss: 1.9044\n",
            "\n",
            "Epoch 00051: loss improved from 1.92043 to 1.90441, saving model to model_weights_saved.hdf5\n",
            "Epoch 52/160\n",
            "9/9 [==============================] - 33s 4s/step - loss: 1.8866\n",
            "\n",
            "Epoch 00052: loss improved from 1.90441 to 1.88656, saving model to model_weights_saved.hdf5\n",
            "Epoch 53/160\n",
            "9/9 [==============================] - 33s 4s/step - loss: 1.8646\n",
            "\n",
            "Epoch 00053: loss improved from 1.88656 to 1.86457, saving model to model_weights_saved.hdf5\n",
            "Epoch 54/160\n",
            "9/9 [==============================] - 33s 4s/step - loss: 1.8475\n",
            "\n",
            "Epoch 00054: loss improved from 1.86457 to 1.84750, saving model to model_weights_saved.hdf5\n",
            "Epoch 55/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.8169\n",
            "\n",
            "Epoch 00055: loss improved from 1.84750 to 1.81693, saving model to model_weights_saved.hdf5\n",
            "Epoch 56/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.8055\n",
            "\n",
            "Epoch 00056: loss improved from 1.81693 to 1.80545, saving model to model_weights_saved.hdf5\n",
            "Epoch 57/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.7843\n",
            "\n",
            "Epoch 00057: loss improved from 1.80545 to 1.78433, saving model to model_weights_saved.hdf5\n",
            "Epoch 58/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.7518\n",
            "\n",
            "Epoch 00058: loss improved from 1.78433 to 1.75180, saving model to model_weights_saved.hdf5\n",
            "Epoch 59/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.7333\n",
            "\n",
            "Epoch 00059: loss improved from 1.75180 to 1.73331, saving model to model_weights_saved.hdf5\n",
            "Epoch 60/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.7168\n",
            "\n",
            "Epoch 00060: loss improved from 1.73331 to 1.71679, saving model to model_weights_saved.hdf5\n",
            "Epoch 61/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.7083\n",
            "\n",
            "Epoch 00061: loss improved from 1.71679 to 1.70835, saving model to model_weights_saved.hdf5\n",
            "Epoch 62/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.6917\n",
            "\n",
            "Epoch 00062: loss improved from 1.70835 to 1.69169, saving model to model_weights_saved.hdf5\n",
            "Epoch 63/160\n",
            "9/9 [==============================] - 33s 4s/step - loss: 1.6533\n",
            "\n",
            "Epoch 00063: loss improved from 1.69169 to 1.65329, saving model to model_weights_saved.hdf5\n",
            "Epoch 64/160\n",
            "9/9 [==============================] - 33s 4s/step - loss: 1.6341\n",
            "\n",
            "Epoch 00064: loss improved from 1.65329 to 1.63412, saving model to model_weights_saved.hdf5\n",
            "Epoch 65/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.6295\n",
            "\n",
            "Epoch 00065: loss improved from 1.63412 to 1.62949, saving model to model_weights_saved.hdf5\n",
            "Epoch 66/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.5961\n",
            "\n",
            "Epoch 00066: loss improved from 1.62949 to 1.59607, saving model to model_weights_saved.hdf5\n",
            "Epoch 67/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.5754\n",
            "\n",
            "Epoch 00067: loss improved from 1.59607 to 1.57542, saving model to model_weights_saved.hdf5\n",
            "Epoch 68/160\n",
            "9/9 [==============================] - 32s 3s/step - loss: 1.5591\n",
            "\n",
            "Epoch 00068: loss improved from 1.57542 to 1.55909, saving model to model_weights_saved.hdf5\n",
            "Epoch 69/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.5645\n",
            "\n",
            "Epoch 00069: loss did not improve from 1.55909\n",
            "Epoch 70/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.5423\n",
            "\n",
            "Epoch 00070: loss improved from 1.55909 to 1.54231, saving model to model_weights_saved.hdf5\n",
            "Epoch 71/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.5153\n",
            "\n",
            "Epoch 00071: loss improved from 1.54231 to 1.51526, saving model to model_weights_saved.hdf5\n",
            "Epoch 72/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.4757\n",
            "\n",
            "Epoch 00072: loss improved from 1.51526 to 1.47568, saving model to model_weights_saved.hdf5\n",
            "Epoch 73/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.4426\n",
            "\n",
            "Epoch 00073: loss improved from 1.47568 to 1.44263, saving model to model_weights_saved.hdf5\n",
            "Epoch 74/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.4485\n",
            "\n",
            "Epoch 00074: loss did not improve from 1.44263\n",
            "Epoch 75/160\n",
            "9/9 [==============================] - 32s 3s/step - loss: 1.4049\n",
            "\n",
            "Epoch 00075: loss improved from 1.44263 to 1.40486, saving model to model_weights_saved.hdf5\n",
            "Epoch 76/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.4068\n",
            "\n",
            "Epoch 00076: loss did not improve from 1.40486\n",
            "Epoch 77/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.3775\n",
            "\n",
            "Epoch 00077: loss improved from 1.40486 to 1.37751, saving model to model_weights_saved.hdf5\n",
            "Epoch 78/160\n",
            "9/9 [==============================] - 32s 3s/step - loss: 1.3497\n",
            "\n",
            "Epoch 00078: loss improved from 1.37751 to 1.34968, saving model to model_weights_saved.hdf5\n",
            "Epoch 79/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.3622\n",
            "\n",
            "Epoch 00079: loss did not improve from 1.34968\n",
            "Epoch 80/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.3295\n",
            "\n",
            "Epoch 00080: loss improved from 1.34968 to 1.32952, saving model to model_weights_saved.hdf5\n",
            "Epoch 81/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.3204\n",
            "\n",
            "Epoch 00081: loss improved from 1.32952 to 1.32037, saving model to model_weights_saved.hdf5\n",
            "Epoch 82/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.2802\n",
            "\n",
            "Epoch 00082: loss improved from 1.32037 to 1.28019, saving model to model_weights_saved.hdf5\n",
            "Epoch 83/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.2584\n",
            "\n",
            "Epoch 00083: loss improved from 1.28019 to 1.25838, saving model to model_weights_saved.hdf5\n",
            "Epoch 84/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 1.2500\n",
            "\n",
            "Epoch 00084: loss improved from 1.25838 to 1.24996, saving model to model_weights_saved.hdf5\n",
            "Epoch 85/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.2386\n",
            "\n",
            "Epoch 00085: loss improved from 1.24996 to 1.23857, saving model to model_weights_saved.hdf5\n",
            "Epoch 86/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.2201\n",
            "\n",
            "Epoch 00086: loss improved from 1.23857 to 1.22007, saving model to model_weights_saved.hdf5\n",
            "Epoch 87/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.2011\n",
            "\n",
            "Epoch 00087: loss improved from 1.22007 to 1.20112, saving model to model_weights_saved.hdf5\n",
            "Epoch 88/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.1808\n",
            "\n",
            "Epoch 00088: loss improved from 1.20112 to 1.18076, saving model to model_weights_saved.hdf5\n",
            "Epoch 89/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.1613\n",
            "\n",
            "Epoch 00089: loss improved from 1.18076 to 1.16132, saving model to model_weights_saved.hdf5\n",
            "Epoch 90/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.1350\n",
            "\n",
            "Epoch 00090: loss improved from 1.16132 to 1.13504, saving model to model_weights_saved.hdf5\n",
            "Epoch 91/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.1245\n",
            "\n",
            "Epoch 00091: loss improved from 1.13504 to 1.12449, saving model to model_weights_saved.hdf5\n",
            "Epoch 92/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.0941\n",
            "\n",
            "Epoch 00092: loss improved from 1.12449 to 1.09408, saving model to model_weights_saved.hdf5\n",
            "Epoch 93/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.0988\n",
            "\n",
            "Epoch 00093: loss did not improve from 1.09408\n",
            "Epoch 94/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.0915\n",
            "\n",
            "Epoch 00094: loss improved from 1.09408 to 1.09150, saving model to model_weights_saved.hdf5\n",
            "Epoch 95/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.0391\n",
            "\n",
            "Epoch 00095: loss improved from 1.09150 to 1.03913, saving model to model_weights_saved.hdf5\n",
            "Epoch 96/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.0260\n",
            "\n",
            "Epoch 00096: loss improved from 1.03913 to 1.02595, saving model to model_weights_saved.hdf5\n",
            "Epoch 97/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 1.0304\n",
            "\n",
            "Epoch 00097: loss did not improve from 1.02595\n",
            "Epoch 98/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 0.9989\n",
            "\n",
            "Epoch 00098: loss improved from 1.02595 to 0.99886, saving model to model_weights_saved.hdf5\n",
            "Epoch 99/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 0.9848\n",
            "\n",
            "Epoch 00099: loss improved from 0.99886 to 0.98475, saving model to model_weights_saved.hdf5\n",
            "Epoch 100/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 0.9599\n",
            "\n",
            "Epoch 00100: loss improved from 0.98475 to 0.95991, saving model to model_weights_saved.hdf5\n",
            "Epoch 101/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.9548\n",
            "\n",
            "Epoch 00101: loss improved from 0.95991 to 0.95480, saving model to model_weights_saved.hdf5\n",
            "Epoch 102/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 0.9460\n",
            "\n",
            "Epoch 00102: loss improved from 0.95480 to 0.94600, saving model to model_weights_saved.hdf5\n",
            "Epoch 103/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 0.9222\n",
            "\n",
            "Epoch 00103: loss improved from 0.94600 to 0.92217, saving model to model_weights_saved.hdf5\n",
            "Epoch 104/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 0.9118\n",
            "\n",
            "Epoch 00104: loss improved from 0.92217 to 0.91180, saving model to model_weights_saved.hdf5\n",
            "Epoch 105/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 0.8823\n",
            "\n",
            "Epoch 00105: loss improved from 0.91180 to 0.88225, saving model to model_weights_saved.hdf5\n",
            "Epoch 106/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 0.8832\n",
            "\n",
            "Epoch 00106: loss did not improve from 0.88225\n",
            "Epoch 107/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.8739\n",
            "\n",
            "Epoch 00107: loss improved from 0.88225 to 0.87387, saving model to model_weights_saved.hdf5\n",
            "Epoch 108/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 0.8528\n",
            "\n",
            "Epoch 00108: loss improved from 0.87387 to 0.85276, saving model to model_weights_saved.hdf5\n",
            "Epoch 109/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.8208\n",
            "\n",
            "Epoch 00109: loss improved from 0.85276 to 0.82079, saving model to model_weights_saved.hdf5\n",
            "Epoch 110/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.8181\n",
            "\n",
            "Epoch 00110: loss improved from 0.82079 to 0.81809, saving model to model_weights_saved.hdf5\n",
            "Epoch 111/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.8002\n",
            "\n",
            "Epoch 00111: loss improved from 0.81809 to 0.80015, saving model to model_weights_saved.hdf5\n",
            "Epoch 112/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 0.7925\n",
            "\n",
            "Epoch 00112: loss improved from 0.80015 to 0.79255, saving model to model_weights_saved.hdf5\n",
            "Epoch 113/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.7824\n",
            "\n",
            "Epoch 00113: loss improved from 0.79255 to 0.78242, saving model to model_weights_saved.hdf5\n",
            "Epoch 114/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.7646\n",
            "\n",
            "Epoch 00114: loss improved from 0.78242 to 0.76459, saving model to model_weights_saved.hdf5\n",
            "Epoch 115/160\n",
            "9/9 [==============================] - 33s 4s/step - loss: 0.7454\n",
            "\n",
            "Epoch 00115: loss improved from 0.76459 to 0.74537, saving model to model_weights_saved.hdf5\n",
            "Epoch 116/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.7418\n",
            "\n",
            "Epoch 00116: loss improved from 0.74537 to 0.74183, saving model to model_weights_saved.hdf5\n",
            "Epoch 117/160\n",
            "9/9 [==============================] - 33s 4s/step - loss: 0.7226\n",
            "\n",
            "Epoch 00117: loss improved from 0.74183 to 0.72256, saving model to model_weights_saved.hdf5\n",
            "Epoch 118/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.7118\n",
            "\n",
            "Epoch 00118: loss improved from 0.72256 to 0.71179, saving model to model_weights_saved.hdf5\n",
            "Epoch 119/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.6893\n",
            "\n",
            "Epoch 00119: loss improved from 0.71179 to 0.68931, saving model to model_weights_saved.hdf5\n",
            "Epoch 120/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.6727\n",
            "\n",
            "Epoch 00120: loss improved from 0.68931 to 0.67273, saving model to model_weights_saved.hdf5\n",
            "Epoch 121/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.6730\n",
            "\n",
            "Epoch 00121: loss did not improve from 0.67273\n",
            "Epoch 122/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 0.6606\n",
            "\n",
            "Epoch 00122: loss improved from 0.67273 to 0.66058, saving model to model_weights_saved.hdf5\n",
            "Epoch 123/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 0.6388\n",
            "\n",
            "Epoch 00123: loss improved from 0.66058 to 0.63878, saving model to model_weights_saved.hdf5\n",
            "Epoch 124/160\n",
            "9/9 [==============================] - 32s 3s/step - loss: 0.6372\n",
            "\n",
            "Epoch 00124: loss improved from 0.63878 to 0.63722, saving model to model_weights_saved.hdf5\n",
            "Epoch 125/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 0.6369\n",
            "\n",
            "Epoch 00125: loss improved from 0.63722 to 0.63686, saving model to model_weights_saved.hdf5\n",
            "Epoch 126/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 0.6239\n",
            "\n",
            "Epoch 00126: loss improved from 0.63686 to 0.62392, saving model to model_weights_saved.hdf5\n",
            "Epoch 127/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.6112\n",
            "\n",
            "Epoch 00127: loss improved from 0.62392 to 0.61123, saving model to model_weights_saved.hdf5\n",
            "Epoch 128/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.5794\n",
            "\n",
            "Epoch 00128: loss improved from 0.61123 to 0.57940, saving model to model_weights_saved.hdf5\n",
            "Epoch 129/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 0.5961\n",
            "\n",
            "Epoch 00129: loss did not improve from 0.57940\n",
            "Epoch 130/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.5764\n",
            "\n",
            "Epoch 00130: loss improved from 0.57940 to 0.57638, saving model to model_weights_saved.hdf5\n",
            "Epoch 131/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.5612\n",
            "\n",
            "Epoch 00131: loss improved from 0.57638 to 0.56119, saving model to model_weights_saved.hdf5\n",
            "Epoch 132/160\n",
            "9/9 [==============================] - 31s 3s/step - loss: 0.5575\n",
            "\n",
            "Epoch 00132: loss improved from 0.56119 to 0.55755, saving model to model_weights_saved.hdf5\n",
            "Epoch 133/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.5452\n",
            "\n",
            "Epoch 00133: loss improved from 0.55755 to 0.54524, saving model to model_weights_saved.hdf5\n",
            "Epoch 134/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.5353\n",
            "\n",
            "Epoch 00134: loss improved from 0.54524 to 0.53526, saving model to model_weights_saved.hdf5\n",
            "Epoch 135/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.5295\n",
            "\n",
            "Epoch 00135: loss improved from 0.53526 to 0.52955, saving model to model_weights_saved.hdf5\n",
            "Epoch 136/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.5068\n",
            "\n",
            "Epoch 00136: loss improved from 0.52955 to 0.50677, saving model to model_weights_saved.hdf5\n",
            "Epoch 137/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.4962\n",
            "\n",
            "Epoch 00137: loss improved from 0.50677 to 0.49620, saving model to model_weights_saved.hdf5\n",
            "Epoch 138/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.4980\n",
            "\n",
            "Epoch 00138: loss did not improve from 0.49620\n",
            "Epoch 139/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.4824\n",
            "\n",
            "Epoch 00139: loss improved from 0.49620 to 0.48238, saving model to model_weights_saved.hdf5\n",
            "Epoch 140/160\n",
            "9/9 [==============================] - 33s 4s/step - loss: 0.4696\n",
            "\n",
            "Epoch 00140: loss improved from 0.48238 to 0.46960, saving model to model_weights_saved.hdf5\n",
            "Epoch 141/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.4788\n",
            "\n",
            "Epoch 00141: loss did not improve from 0.46960\n",
            "Epoch 142/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.4661\n",
            "\n",
            "Epoch 00142: loss improved from 0.46960 to 0.46612, saving model to model_weights_saved.hdf5\n",
            "Epoch 143/160\n",
            "9/9 [==============================] - 32s 3s/step - loss: 0.4654\n",
            "\n",
            "Epoch 00143: loss improved from 0.46612 to 0.46537, saving model to model_weights_saved.hdf5\n",
            "Epoch 144/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.4512\n",
            "\n",
            "Epoch 00144: loss improved from 0.46537 to 0.45119, saving model to model_weights_saved.hdf5\n",
            "Epoch 145/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.4457\n",
            "\n",
            "Epoch 00145: loss improved from 0.45119 to 0.44565, saving model to model_weights_saved.hdf5\n",
            "Epoch 146/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.4495\n",
            "\n",
            "Epoch 00146: loss did not improve from 0.44565\n",
            "Epoch 147/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.4149\n",
            "\n",
            "Epoch 00147: loss improved from 0.44565 to 0.41493, saving model to model_weights_saved.hdf5\n",
            "Epoch 148/160\n",
            "9/9 [==============================] - 33s 4s/step - loss: 0.4061\n",
            "\n",
            "Epoch 00148: loss improved from 0.41493 to 0.40605, saving model to model_weights_saved.hdf5\n",
            "Epoch 149/160\n",
            "9/9 [==============================] - 34s 4s/step - loss: 0.4175\n",
            "\n",
            "Epoch 00149: loss did not improve from 0.40605\n",
            "Epoch 150/160\n",
            "9/9 [==============================] - 33s 4s/step - loss: 0.4195\n",
            "\n",
            "Epoch 00150: loss did not improve from 0.40605\n",
            "Epoch 151/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.3950\n",
            "\n",
            "Epoch 00151: loss improved from 0.40605 to 0.39502, saving model to model_weights_saved.hdf5\n",
            "Epoch 152/160\n",
            "9/9 [==============================] - 33s 4s/step - loss: 0.3947\n",
            "\n",
            "Epoch 00152: loss improved from 0.39502 to 0.39471, saving model to model_weights_saved.hdf5\n",
            "Epoch 153/160\n",
            "9/9 [==============================] - 33s 4s/step - loss: 0.3809\n",
            "\n",
            "Epoch 00153: loss improved from 0.39471 to 0.38087, saving model to model_weights_saved.hdf5\n",
            "Epoch 154/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.3937\n",
            "\n",
            "Epoch 00154: loss did not improve from 0.38087\n",
            "Epoch 155/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.3638\n",
            "\n",
            "Epoch 00155: loss improved from 0.38087 to 0.36381, saving model to model_weights_saved.hdf5\n",
            "Epoch 156/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.3625\n",
            "\n",
            "Epoch 00156: loss improved from 0.36381 to 0.36247, saving model to model_weights_saved.hdf5\n",
            "Epoch 157/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.3441\n",
            "\n",
            "Epoch 00157: loss improved from 0.36247 to 0.34412, saving model to model_weights_saved.hdf5\n",
            "Epoch 158/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.3358\n",
            "\n",
            "Epoch 00158: loss improved from 0.34412 to 0.33584, saving model to model_weights_saved.hdf5\n",
            "Epoch 159/160\n",
            "9/9 [==============================] - 32s 4s/step - loss: 0.3350\n",
            "\n",
            "Epoch 00159: loss improved from 0.33584 to 0.33497, saving model to model_weights_saved.hdf5\n",
            "Epoch 160/160\n",
            "9/9 [==============================] - 32s 3s/step - loss: 0.3412\n",
            "\n",
            "Epoch 00160: loss did not improve from 0.33497\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3a676f30d0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrs4PFDZPEtH"
      },
      "source": [
        "filename = \"model_weights_saved.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Asdd0dD_PEwv"
      },
      "source": [
        "num_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJsLfWd0QAw8",
        "outputId": "4ab732d6-b7f9-48f1-bed8-219e55039d3a"
      },
      "source": [
        "start = numpy.random.randint(0, len(x_data) - 1)\n",
        "pattern = x_data[start]\n",
        "print(\"Random Seed:\")\n",
        "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed:\n",
            "\" on led many governments classify weapon limit even prohibit use export 6 jurisdictions use cryptogra \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "KMS4LzaiQA3P",
        "outputId": "ab805a32-8621-4789-bbd4-22d526a52393"
      },
      "source": [
        "# generate the text\n",
        "for i in range(1000):\n",
        "  x = numpy.reshape(pattern, (1,len(pattern), 1))\n",
        "  x = x/float(vocab_len)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = num_to_char[index]\n",
        "  seq_in = [num_to_char[value] for value in pattern]\n",
        "  sys.stdout.write(result)\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[ 1:len(pattern)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "phy letal palm designed therefore termed computationally secure schemes provably cannot broken even unlimited computing power one time pad schemes much difficult use practice best theoretically breakable computationally secure schemes provably cannot broken even unlimited computing power one time pad schemes much difficult use practice best theoretically breakable computationally secure schemes provably cannot broken even unlimited computing power one time pad schemes much difficult use practice best theoretically breakable computationally secure schemes provably cannot broken even unlimited computing power one time pad schemes much difficult use practice best theoretically breakable computationally secure s"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-51fa7be13319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1749\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}