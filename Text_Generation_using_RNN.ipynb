{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation using RNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNiOfyUhgXCTuSspyYZTv+h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aashish1106/Text-Generation/blob/test/Text_Generation_using_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg0bhBp7azr4"
      },
      "source": [
        "**Text Generation** using a **Recurrent Neural Networks**, specifically a **Long Short-Term Memory Network**, implementing this network in Python, and use it to generate some text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Opra2UQKTnRA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f398b489-b2c1-40a7-cd6c-2148d752a0d1"
      },
      "source": [
        "import numpy\n",
        "import sys\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIUf206xTnYb"
      },
      "source": [
        "file = open(\"New.txt\").read()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1THFMhZnTnd_"
      },
      "source": [
        "def tokenize_words(input):\n",
        "    # lowercase everything to standardize it\n",
        "    input = input.lower()\n",
        "\n",
        "    # instantiate the tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(input)\n",
        "\n",
        "    # if the created token isn't in the stop words, make it part of \"filtered\"\n",
        "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
        "    return \" \".join(filtered)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSQEIQMsIXHL"
      },
      "source": [
        "# preprocess the input data, make tokens\n",
        "processed_inputs = tokenize_words(file)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z56LmkfIXfT"
      },
      "source": [
        "chars = sorted(list(set(processed_inputs)))\n",
        "char_to_num = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwT220aUIXls",
        "outputId": "b6ba07b1-c8ad-4d62-d3df-9d1f9ddbc2f3"
      },
      "source": [
        "input_len = len(processed_inputs)\n",
        "vocab_len = len(chars)\n",
        "print (\"Total number of characters:\", input_len)\n",
        "print (\"Total vocab:\", vocab_len)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 2376\n",
            "Total vocab: 53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxYVlNdlIXqE"
      },
      "source": [
        "seq_length = 100\n",
        "x_data = []\n",
        "y_data = []"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgtxAmSqIXuM"
      },
      "source": [
        "# loop through inputs, start at the beginning and go until we hit\n",
        "# the final character we can create a sequence out of\n",
        "for i in range(0, input_len - seq_length, 1):\n",
        "    # Define input and output sequences\n",
        "    # Input is the current character plus desired sequence length\n",
        "    in_seq = processed_inputs[i:i + seq_length]\n",
        "\n",
        "    # Out sequence is the initial character plus total sequence length\n",
        "    out_seq = processed_inputs[i + seq_length]\n",
        "\n",
        "    # We now convert list of characters to integers based on\n",
        "    # previously and add the values to our lists\n",
        "    x_data.append([char_to_num[char] for char in in_seq])\n",
        "    y_data.append(char_to_num[out_seq])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFlxDwSWPElh",
        "outputId": "06f6b7c0-7367-4d6c-a833-857fbf3f2342"
      },
      "source": [
        "n_patterns = len(x_data)\n",
        "print (\"Total Patterns:\", n_patterns)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns: 2276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P5tcT7bPJn3"
      },
      "source": [
        "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
        "X = X/float(vocab_len)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3wBB0f1PJwm"
      },
      "source": [
        "y = np_utils.to_categorical(y_data)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nebCgEvIPOyf"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4LG0YoPPO2H"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JfR5gYiPWPl"
      },
      "source": [
        "filepath = \"model_weights_saved.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "desired_callbacks = [checkpoint]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T696XWPUPZgt",
        "outputId": "081011e7-9b41-4441-8120-d0f7a1180e68"
      },
      "source": [
        "model.fit(X, y, epochs=205, batch_size=256, callbacks=desired_callbacks)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/205\n",
            "9/9 [==============================] - 31s 3s/step - loss: 3.4769\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.47691, saving model to model_weights_saved.hdf5\n",
            "Epoch 2/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 3.0860\n",
            "\n",
            "Epoch 00002: loss improved from 3.47691 to 3.08604, saving model to model_weights_saved.hdf5\n",
            "Epoch 3/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 3.0418\n",
            "\n",
            "Epoch 00003: loss improved from 3.08604 to 3.04175, saving model to model_weights_saved.hdf5\n",
            "Epoch 4/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 3.0274\n",
            "\n",
            "Epoch 00004: loss improved from 3.04175 to 3.02735, saving model to model_weights_saved.hdf5\n",
            "Epoch 5/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9988\n",
            "\n",
            "Epoch 00005: loss improved from 3.02735 to 2.99880, saving model to model_weights_saved.hdf5\n",
            "Epoch 6/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9932\n",
            "\n",
            "Epoch 00006: loss improved from 2.99880 to 2.99318, saving model to model_weights_saved.hdf5\n",
            "Epoch 7/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9940\n",
            "\n",
            "Epoch 00007: loss did not improve from 2.99318\n",
            "Epoch 8/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9981\n",
            "\n",
            "Epoch 00008: loss did not improve from 2.99318\n",
            "Epoch 9/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9901\n",
            "\n",
            "Epoch 00009: loss improved from 2.99318 to 2.99009, saving model to model_weights_saved.hdf5\n",
            "Epoch 10/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9798\n",
            "\n",
            "Epoch 00010: loss improved from 2.99009 to 2.97985, saving model to model_weights_saved.hdf5\n",
            "Epoch 11/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9792\n",
            "\n",
            "Epoch 00011: loss improved from 2.97985 to 2.97916, saving model to model_weights_saved.hdf5\n",
            "Epoch 12/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9813\n",
            "\n",
            "Epoch 00012: loss did not improve from 2.97916\n",
            "Epoch 13/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9820\n",
            "\n",
            "Epoch 00013: loss did not improve from 2.97916\n",
            "Epoch 14/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9838\n",
            "\n",
            "Epoch 00014: loss did not improve from 2.97916\n",
            "Epoch 15/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9794\n",
            "\n",
            "Epoch 00015: loss did not improve from 2.97916\n",
            "Epoch 16/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9706\n",
            "\n",
            "Epoch 00016: loss improved from 2.97916 to 2.97057, saving model to model_weights_saved.hdf5\n",
            "Epoch 17/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9747\n",
            "\n",
            "Epoch 00017: loss did not improve from 2.97057\n",
            "Epoch 18/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9686\n",
            "\n",
            "Epoch 00018: loss improved from 2.97057 to 2.96862, saving model to model_weights_saved.hdf5\n",
            "Epoch 19/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9765\n",
            "\n",
            "Epoch 00019: loss did not improve from 2.96862\n",
            "Epoch 20/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9716\n",
            "\n",
            "Epoch 00020: loss did not improve from 2.96862\n",
            "Epoch 21/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9714\n",
            "\n",
            "Epoch 00021: loss did not improve from 2.96862\n",
            "Epoch 22/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9599\n",
            "\n",
            "Epoch 00022: loss improved from 2.96862 to 2.95989, saving model to model_weights_saved.hdf5\n",
            "Epoch 23/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9737\n",
            "\n",
            "Epoch 00023: loss did not improve from 2.95989\n",
            "Epoch 24/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9653\n",
            "\n",
            "Epoch 00024: loss did not improve from 2.95989\n",
            "Epoch 25/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9609\n",
            "\n",
            "Epoch 00025: loss did not improve from 2.95989\n",
            "Epoch 26/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9602\n",
            "\n",
            "Epoch 00026: loss did not improve from 2.95989\n",
            "Epoch 27/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9639\n",
            "\n",
            "Epoch 00027: loss did not improve from 2.95989\n",
            "Epoch 28/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9657\n",
            "\n",
            "Epoch 00028: loss did not improve from 2.95989\n",
            "Epoch 29/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9623\n",
            "\n",
            "Epoch 00029: loss did not improve from 2.95989\n",
            "Epoch 30/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9669\n",
            "\n",
            "Epoch 00030: loss did not improve from 2.95989\n",
            "Epoch 31/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9603\n",
            "\n",
            "Epoch 00031: loss did not improve from 2.95989\n",
            "Epoch 32/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9504\n",
            "\n",
            "Epoch 00032: loss improved from 2.95989 to 2.95041, saving model to model_weights_saved.hdf5\n",
            "Epoch 33/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9523\n",
            "\n",
            "Epoch 00033: loss did not improve from 2.95041\n",
            "Epoch 34/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9568\n",
            "\n",
            "Epoch 00034: loss did not improve from 2.95041\n",
            "Epoch 35/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9574\n",
            "\n",
            "Epoch 00035: loss did not improve from 2.95041\n",
            "Epoch 36/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9605\n",
            "\n",
            "Epoch 00036: loss did not improve from 2.95041\n",
            "Epoch 37/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9549\n",
            "\n",
            "Epoch 00037: loss did not improve from 2.95041\n",
            "Epoch 38/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9524\n",
            "\n",
            "Epoch 00038: loss did not improve from 2.95041\n",
            "Epoch 39/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9563\n",
            "\n",
            "Epoch 00039: loss did not improve from 2.95041\n",
            "Epoch 40/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9528\n",
            "\n",
            "Epoch 00040: loss did not improve from 2.95041\n",
            "Epoch 41/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9554\n",
            "\n",
            "Epoch 00041: loss did not improve from 2.95041\n",
            "Epoch 42/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9566\n",
            "\n",
            "Epoch 00042: loss did not improve from 2.95041\n",
            "Epoch 43/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9579\n",
            "\n",
            "Epoch 00043: loss did not improve from 2.95041\n",
            "Epoch 44/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9512\n",
            "\n",
            "Epoch 00044: loss did not improve from 2.95041\n",
            "Epoch 45/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9540\n",
            "\n",
            "Epoch 00045: loss did not improve from 2.95041\n",
            "Epoch 46/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9517\n",
            "\n",
            "Epoch 00046: loss did not improve from 2.95041\n",
            "Epoch 47/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9578\n",
            "\n",
            "Epoch 00047: loss did not improve from 2.95041\n",
            "Epoch 48/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9526\n",
            "\n",
            "Epoch 00048: loss did not improve from 2.95041\n",
            "Epoch 49/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9611\n",
            "\n",
            "Epoch 00049: loss did not improve from 2.95041\n",
            "Epoch 50/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9547\n",
            "\n",
            "Epoch 00050: loss did not improve from 2.95041\n",
            "Epoch 51/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9560\n",
            "\n",
            "Epoch 00051: loss did not improve from 2.95041\n",
            "Epoch 52/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9443\n",
            "\n",
            "Epoch 00052: loss improved from 2.95041 to 2.94426, saving model to model_weights_saved.hdf5\n",
            "Epoch 53/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9508\n",
            "\n",
            "Epoch 00053: loss did not improve from 2.94426\n",
            "Epoch 54/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9508\n",
            "\n",
            "Epoch 00054: loss did not improve from 2.94426\n",
            "Epoch 55/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9507\n",
            "\n",
            "Epoch 00055: loss did not improve from 2.94426\n",
            "Epoch 56/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9447\n",
            "\n",
            "Epoch 00056: loss did not improve from 2.94426\n",
            "Epoch 57/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9496\n",
            "\n",
            "Epoch 00057: loss did not improve from 2.94426\n",
            "Epoch 58/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9550\n",
            "\n",
            "Epoch 00058: loss did not improve from 2.94426\n",
            "Epoch 59/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9497\n",
            "\n",
            "Epoch 00059: loss did not improve from 2.94426\n",
            "Epoch 60/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9506\n",
            "\n",
            "Epoch 00060: loss did not improve from 2.94426\n",
            "Epoch 61/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9533\n",
            "\n",
            "Epoch 00061: loss did not improve from 2.94426\n",
            "Epoch 62/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9537\n",
            "\n",
            "Epoch 00062: loss did not improve from 2.94426\n",
            "Epoch 63/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9507\n",
            "\n",
            "Epoch 00063: loss did not improve from 2.94426\n",
            "Epoch 64/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9541\n",
            "\n",
            "Epoch 00064: loss did not improve from 2.94426\n",
            "Epoch 65/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9531\n",
            "\n",
            "Epoch 00065: loss did not improve from 2.94426\n",
            "Epoch 66/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9548\n",
            "\n",
            "Epoch 00066: loss did not improve from 2.94426\n",
            "Epoch 67/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9463\n",
            "\n",
            "Epoch 00067: loss did not improve from 2.94426\n",
            "Epoch 68/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9489\n",
            "\n",
            "Epoch 00068: loss did not improve from 2.94426\n",
            "Epoch 69/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9488\n",
            "\n",
            "Epoch 00069: loss did not improve from 2.94426\n",
            "Epoch 70/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9524\n",
            "\n",
            "Epoch 00070: loss did not improve from 2.94426\n",
            "Epoch 71/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9480\n",
            "\n",
            "Epoch 00071: loss did not improve from 2.94426\n",
            "Epoch 72/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9456\n",
            "\n",
            "Epoch 00072: loss did not improve from 2.94426\n",
            "Epoch 73/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9480\n",
            "\n",
            "Epoch 00073: loss did not improve from 2.94426\n",
            "Epoch 74/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9482\n",
            "\n",
            "Epoch 00074: loss did not improve from 2.94426\n",
            "Epoch 75/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9492\n",
            "\n",
            "Epoch 00075: loss did not improve from 2.94426\n",
            "Epoch 76/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9505\n",
            "\n",
            "Epoch 00076: loss did not improve from 2.94426\n",
            "Epoch 77/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9494\n",
            "\n",
            "Epoch 00077: loss did not improve from 2.94426\n",
            "Epoch 78/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9469\n",
            "\n",
            "Epoch 00078: loss did not improve from 2.94426\n",
            "Epoch 79/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9505\n",
            "\n",
            "Epoch 00079: loss did not improve from 2.94426\n",
            "Epoch 80/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9482\n",
            "\n",
            "Epoch 00080: loss did not improve from 2.94426\n",
            "Epoch 81/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9512\n",
            "\n",
            "Epoch 00081: loss did not improve from 2.94426\n",
            "Epoch 82/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9484\n",
            "\n",
            "Epoch 00082: loss did not improve from 2.94426\n",
            "Epoch 83/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9442\n",
            "\n",
            "Epoch 00083: loss improved from 2.94426 to 2.94419, saving model to model_weights_saved.hdf5\n",
            "Epoch 84/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.9454\n",
            "\n",
            "Epoch 00084: loss did not improve from 2.94419\n",
            "Epoch 85/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9459\n",
            "\n",
            "Epoch 00085: loss did not improve from 2.94419\n",
            "Epoch 86/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9416\n",
            "\n",
            "Epoch 00086: loss improved from 2.94419 to 2.94155, saving model to model_weights_saved.hdf5\n",
            "Epoch 87/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9422\n",
            "\n",
            "Epoch 00087: loss did not improve from 2.94155\n",
            "Epoch 88/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9451\n",
            "\n",
            "Epoch 00088: loss did not improve from 2.94155\n",
            "Epoch 89/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9448\n",
            "\n",
            "Epoch 00089: loss did not improve from 2.94155\n",
            "Epoch 90/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9432\n",
            "\n",
            "Epoch 00090: loss did not improve from 2.94155\n",
            "Epoch 91/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9453\n",
            "\n",
            "Epoch 00091: loss did not improve from 2.94155\n",
            "Epoch 92/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9444\n",
            "\n",
            "Epoch 00092: loss did not improve from 2.94155\n",
            "Epoch 93/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9419\n",
            "\n",
            "Epoch 00093: loss did not improve from 2.94155\n",
            "Epoch 94/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9369\n",
            "\n",
            "Epoch 00094: loss improved from 2.94155 to 2.93694, saving model to model_weights_saved.hdf5\n",
            "Epoch 95/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9417\n",
            "\n",
            "Epoch 00095: loss did not improve from 2.93694\n",
            "Epoch 96/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9342\n",
            "\n",
            "Epoch 00096: loss improved from 2.93694 to 2.93420, saving model to model_weights_saved.hdf5\n",
            "Epoch 97/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9282\n",
            "\n",
            "Epoch 00097: loss improved from 2.93420 to 2.92817, saving model to model_weights_saved.hdf5\n",
            "Epoch 98/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9238\n",
            "\n",
            "Epoch 00098: loss improved from 2.92817 to 2.92383, saving model to model_weights_saved.hdf5\n",
            "Epoch 99/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9289\n",
            "\n",
            "Epoch 00099: loss did not improve from 2.92383\n",
            "Epoch 100/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9118\n",
            "\n",
            "Epoch 00100: loss improved from 2.92383 to 2.91178, saving model to model_weights_saved.hdf5\n",
            "Epoch 101/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9162\n",
            "\n",
            "Epoch 00101: loss did not improve from 2.91178\n",
            "Epoch 102/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9105\n",
            "\n",
            "Epoch 00102: loss improved from 2.91178 to 2.91055, saving model to model_weights_saved.hdf5\n",
            "Epoch 103/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9006\n",
            "\n",
            "Epoch 00103: loss improved from 2.91055 to 2.90059, saving model to model_weights_saved.hdf5\n",
            "Epoch 104/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.8992\n",
            "\n",
            "Epoch 00104: loss improved from 2.90059 to 2.89923, saving model to model_weights_saved.hdf5\n",
            "Epoch 105/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.9067\n",
            "\n",
            "Epoch 00105: loss did not improve from 2.89923\n",
            "Epoch 106/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.8940\n",
            "\n",
            "Epoch 00106: loss improved from 2.89923 to 2.89398, saving model to model_weights_saved.hdf5\n",
            "Epoch 107/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.8860\n",
            "\n",
            "Epoch 00107: loss improved from 2.89398 to 2.88598, saving model to model_weights_saved.hdf5\n",
            "Epoch 108/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.8822\n",
            "\n",
            "Epoch 00108: loss improved from 2.88598 to 2.88219, saving model to model_weights_saved.hdf5\n",
            "Epoch 109/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.8844\n",
            "\n",
            "Epoch 00109: loss did not improve from 2.88219\n",
            "Epoch 110/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.8763\n",
            "\n",
            "Epoch 00110: loss improved from 2.88219 to 2.87630, saving model to model_weights_saved.hdf5\n",
            "Epoch 111/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.8729\n",
            "\n",
            "Epoch 00111: loss improved from 2.87630 to 2.87292, saving model to model_weights_saved.hdf5\n",
            "Epoch 112/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.8612\n",
            "\n",
            "Epoch 00112: loss improved from 2.87292 to 2.86115, saving model to model_weights_saved.hdf5\n",
            "Epoch 113/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.8636\n",
            "\n",
            "Epoch 00113: loss did not improve from 2.86115\n",
            "Epoch 114/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.8526\n",
            "\n",
            "Epoch 00114: loss improved from 2.86115 to 2.85258, saving model to model_weights_saved.hdf5\n",
            "Epoch 115/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.8554\n",
            "\n",
            "Epoch 00115: loss did not improve from 2.85258\n",
            "Epoch 116/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.8409\n",
            "\n",
            "Epoch 00116: loss improved from 2.85258 to 2.84092, saving model to model_weights_saved.hdf5\n",
            "Epoch 117/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.8368\n",
            "\n",
            "Epoch 00117: loss improved from 2.84092 to 2.83684, saving model to model_weights_saved.hdf5\n",
            "Epoch 118/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.8395\n",
            "\n",
            "Epoch 00118: loss did not improve from 2.83684\n",
            "Epoch 119/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.8236\n",
            "\n",
            "Epoch 00119: loss improved from 2.83684 to 2.82358, saving model to model_weights_saved.hdf5\n",
            "Epoch 120/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.8285\n",
            "\n",
            "Epoch 00120: loss did not improve from 2.82358\n",
            "Epoch 121/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.8131\n",
            "\n",
            "Epoch 00121: loss improved from 2.82358 to 2.81308, saving model to model_weights_saved.hdf5\n",
            "Epoch 122/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.8051\n",
            "\n",
            "Epoch 00122: loss improved from 2.81308 to 2.80505, saving model to model_weights_saved.hdf5\n",
            "Epoch 123/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.7944\n",
            "\n",
            "Epoch 00123: loss improved from 2.80505 to 2.79441, saving model to model_weights_saved.hdf5\n",
            "Epoch 124/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.7875\n",
            "\n",
            "Epoch 00124: loss improved from 2.79441 to 2.78745, saving model to model_weights_saved.hdf5\n",
            "Epoch 125/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.7745\n",
            "\n",
            "Epoch 00125: loss improved from 2.78745 to 2.77445, saving model to model_weights_saved.hdf5\n",
            "Epoch 126/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.7595\n",
            "\n",
            "Epoch 00126: loss improved from 2.77445 to 2.75955, saving model to model_weights_saved.hdf5\n",
            "Epoch 127/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.7506\n",
            "\n",
            "Epoch 00127: loss improved from 2.75955 to 2.75059, saving model to model_weights_saved.hdf5\n",
            "Epoch 128/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.7431\n",
            "\n",
            "Epoch 00128: loss improved from 2.75059 to 2.74311, saving model to model_weights_saved.hdf5\n",
            "Epoch 129/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.7407\n",
            "\n",
            "Epoch 00129: loss improved from 2.74311 to 2.74071, saving model to model_weights_saved.hdf5\n",
            "Epoch 130/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.7204\n",
            "\n",
            "Epoch 00130: loss improved from 2.74071 to 2.72043, saving model to model_weights_saved.hdf5\n",
            "Epoch 131/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.6970\n",
            "\n",
            "Epoch 00131: loss improved from 2.72043 to 2.69696, saving model to model_weights_saved.hdf5\n",
            "Epoch 132/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.6718\n",
            "\n",
            "Epoch 00132: loss improved from 2.69696 to 2.67176, saving model to model_weights_saved.hdf5\n",
            "Epoch 133/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.6471\n",
            "\n",
            "Epoch 00133: loss improved from 2.67176 to 2.64714, saving model to model_weights_saved.hdf5\n",
            "Epoch 134/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.6405\n",
            "\n",
            "Epoch 00134: loss improved from 2.64714 to 2.64052, saving model to model_weights_saved.hdf5\n",
            "Epoch 135/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.6092\n",
            "\n",
            "Epoch 00135: loss improved from 2.64052 to 2.60919, saving model to model_weights_saved.hdf5\n",
            "Epoch 136/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.5892\n",
            "\n",
            "Epoch 00136: loss improved from 2.60919 to 2.58924, saving model to model_weights_saved.hdf5\n",
            "Epoch 137/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.5744\n",
            "\n",
            "Epoch 00137: loss improved from 2.58924 to 2.57442, saving model to model_weights_saved.hdf5\n",
            "Epoch 138/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.5627\n",
            "\n",
            "Epoch 00138: loss improved from 2.57442 to 2.56270, saving model to model_weights_saved.hdf5\n",
            "Epoch 139/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.5330\n",
            "\n",
            "Epoch 00139: loss improved from 2.56270 to 2.53302, saving model to model_weights_saved.hdf5\n",
            "Epoch 140/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.5107\n",
            "\n",
            "Epoch 00140: loss improved from 2.53302 to 2.51066, saving model to model_weights_saved.hdf5\n",
            "Epoch 141/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.4982\n",
            "\n",
            "Epoch 00141: loss improved from 2.51066 to 2.49823, saving model to model_weights_saved.hdf5\n",
            "Epoch 142/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.4892\n",
            "\n",
            "Epoch 00142: loss improved from 2.49823 to 2.48919, saving model to model_weights_saved.hdf5\n",
            "Epoch 143/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.4711\n",
            "\n",
            "Epoch 00143: loss improved from 2.48919 to 2.47115, saving model to model_weights_saved.hdf5\n",
            "Epoch 144/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.4377\n",
            "\n",
            "Epoch 00144: loss improved from 2.47115 to 2.43772, saving model to model_weights_saved.hdf5\n",
            "Epoch 145/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.4352\n",
            "\n",
            "Epoch 00145: loss improved from 2.43772 to 2.43518, saving model to model_weights_saved.hdf5\n",
            "Epoch 146/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.3908\n",
            "\n",
            "Epoch 00146: loss improved from 2.43518 to 2.39078, saving model to model_weights_saved.hdf5\n",
            "Epoch 147/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.3797\n",
            "\n",
            "Epoch 00147: loss improved from 2.39078 to 2.37971, saving model to model_weights_saved.hdf5\n",
            "Epoch 148/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.3703\n",
            "\n",
            "Epoch 00148: loss improved from 2.37971 to 2.37033, saving model to model_weights_saved.hdf5\n",
            "Epoch 149/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.3477\n",
            "\n",
            "Epoch 00149: loss improved from 2.37033 to 2.34770, saving model to model_weights_saved.hdf5\n",
            "Epoch 150/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.3406\n",
            "\n",
            "Epoch 00150: loss improved from 2.34770 to 2.34060, saving model to model_weights_saved.hdf5\n",
            "Epoch 151/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.3238\n",
            "\n",
            "Epoch 00151: loss improved from 2.34060 to 2.32381, saving model to model_weights_saved.hdf5\n",
            "Epoch 152/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.2997\n",
            "\n",
            "Epoch 00152: loss improved from 2.32381 to 2.29972, saving model to model_weights_saved.hdf5\n",
            "Epoch 153/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.2758\n",
            "\n",
            "Epoch 00153: loss improved from 2.29972 to 2.27582, saving model to model_weights_saved.hdf5\n",
            "Epoch 154/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.2766\n",
            "\n",
            "Epoch 00154: loss did not improve from 2.27582\n",
            "Epoch 155/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.2507\n",
            "\n",
            "Epoch 00155: loss improved from 2.27582 to 2.25068, saving model to model_weights_saved.hdf5\n",
            "Epoch 156/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.2434\n",
            "\n",
            "Epoch 00156: loss improved from 2.25068 to 2.24341, saving model to model_weights_saved.hdf5\n",
            "Epoch 157/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.2225\n",
            "\n",
            "Epoch 00157: loss improved from 2.24341 to 2.22247, saving model to model_weights_saved.hdf5\n",
            "Epoch 158/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.2071\n",
            "\n",
            "Epoch 00158: loss improved from 2.22247 to 2.20714, saving model to model_weights_saved.hdf5\n",
            "Epoch 159/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.1908\n",
            "\n",
            "Epoch 00159: loss improved from 2.20714 to 2.19078, saving model to model_weights_saved.hdf5\n",
            "Epoch 160/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.1552\n",
            "\n",
            "Epoch 00160: loss improved from 2.19078 to 2.15524, saving model to model_weights_saved.hdf5\n",
            "Epoch 161/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.1544\n",
            "\n",
            "Epoch 00161: loss improved from 2.15524 to 2.15444, saving model to model_weights_saved.hdf5\n",
            "Epoch 162/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 2.1489\n",
            "\n",
            "Epoch 00162: loss improved from 2.15444 to 2.14886, saving model to model_weights_saved.hdf5\n",
            "Epoch 163/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.1248\n",
            "\n",
            "Epoch 00163: loss improved from 2.14886 to 2.12483, saving model to model_weights_saved.hdf5\n",
            "Epoch 164/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.1172\n",
            "\n",
            "Epoch 00164: loss improved from 2.12483 to 2.11723, saving model to model_weights_saved.hdf5\n",
            "Epoch 165/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.0827\n",
            "\n",
            "Epoch 00165: loss improved from 2.11723 to 2.08275, saving model to model_weights_saved.hdf5\n",
            "Epoch 166/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.0599\n",
            "\n",
            "Epoch 00166: loss improved from 2.08275 to 2.05988, saving model to model_weights_saved.hdf5\n",
            "Epoch 167/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.0464\n",
            "\n",
            "Epoch 00167: loss improved from 2.05988 to 2.04637, saving model to model_weights_saved.hdf5\n",
            "Epoch 168/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.0379\n",
            "\n",
            "Epoch 00168: loss improved from 2.04637 to 2.03787, saving model to model_weights_saved.hdf5\n",
            "Epoch 169/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.0221\n",
            "\n",
            "Epoch 00169: loss improved from 2.03787 to 2.02214, saving model to model_weights_saved.hdf5\n",
            "Epoch 170/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 2.0004\n",
            "\n",
            "Epoch 00170: loss improved from 2.02214 to 2.00036, saving model to model_weights_saved.hdf5\n",
            "Epoch 171/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.9936\n",
            "\n",
            "Epoch 00171: loss improved from 2.00036 to 1.99362, saving model to model_weights_saved.hdf5\n",
            "Epoch 172/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.9866\n",
            "\n",
            "Epoch 00172: loss improved from 1.99362 to 1.98663, saving model to model_weights_saved.hdf5\n",
            "Epoch 173/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.9498\n",
            "\n",
            "Epoch 00173: loss improved from 1.98663 to 1.94979, saving model to model_weights_saved.hdf5\n",
            "Epoch 174/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.9184\n",
            "\n",
            "Epoch 00174: loss improved from 1.94979 to 1.91840, saving model to model_weights_saved.hdf5\n",
            "Epoch 175/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.8896\n",
            "\n",
            "Epoch 00175: loss improved from 1.91840 to 1.88965, saving model to model_weights_saved.hdf5\n",
            "Epoch 176/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.9045\n",
            "\n",
            "Epoch 00176: loss did not improve from 1.88965\n",
            "Epoch 177/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.8666\n",
            "\n",
            "Epoch 00177: loss improved from 1.88965 to 1.86664, saving model to model_weights_saved.hdf5\n",
            "Epoch 178/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.8406\n",
            "\n",
            "Epoch 00178: loss improved from 1.86664 to 1.84064, saving model to model_weights_saved.hdf5\n",
            "Epoch 179/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.8198\n",
            "\n",
            "Epoch 00179: loss improved from 1.84064 to 1.81981, saving model to model_weights_saved.hdf5\n",
            "Epoch 180/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.8032\n",
            "\n",
            "Epoch 00180: loss improved from 1.81981 to 1.80315, saving model to model_weights_saved.hdf5\n",
            "Epoch 181/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.8070\n",
            "\n",
            "Epoch 00181: loss did not improve from 1.80315\n",
            "Epoch 182/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.7804\n",
            "\n",
            "Epoch 00182: loss improved from 1.80315 to 1.78045, saving model to model_weights_saved.hdf5\n",
            "Epoch 183/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.7700\n",
            "\n",
            "Epoch 00183: loss improved from 1.78045 to 1.77004, saving model to model_weights_saved.hdf5\n",
            "Epoch 184/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.7579\n",
            "\n",
            "Epoch 00184: loss improved from 1.77004 to 1.75787, saving model to model_weights_saved.hdf5\n",
            "Epoch 185/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.7461\n",
            "\n",
            "Epoch 00185: loss improved from 1.75787 to 1.74606, saving model to model_weights_saved.hdf5\n",
            "Epoch 186/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.7289\n",
            "\n",
            "Epoch 00186: loss improved from 1.74606 to 1.72892, saving model to model_weights_saved.hdf5\n",
            "Epoch 187/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.6828\n",
            "\n",
            "Epoch 00187: loss improved from 1.72892 to 1.68278, saving model to model_weights_saved.hdf5\n",
            "Epoch 188/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.6579\n",
            "\n",
            "Epoch 00188: loss improved from 1.68278 to 1.65789, saving model to model_weights_saved.hdf5\n",
            "Epoch 189/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.6433\n",
            "\n",
            "Epoch 00189: loss improved from 1.65789 to 1.64327, saving model to model_weights_saved.hdf5\n",
            "Epoch 190/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.6280\n",
            "\n",
            "Epoch 00190: loss improved from 1.64327 to 1.62796, saving model to model_weights_saved.hdf5\n",
            "Epoch 191/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.5912\n",
            "\n",
            "Epoch 00191: loss improved from 1.62796 to 1.59122, saving model to model_weights_saved.hdf5\n",
            "Epoch 192/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.5822\n",
            "\n",
            "Epoch 00192: loss improved from 1.59122 to 1.58221, saving model to model_weights_saved.hdf5\n",
            "Epoch 193/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.5595\n",
            "\n",
            "Epoch 00193: loss improved from 1.58221 to 1.55950, saving model to model_weights_saved.hdf5\n",
            "Epoch 194/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.5778\n",
            "\n",
            "Epoch 00194: loss did not improve from 1.55950\n",
            "Epoch 195/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.5409\n",
            "\n",
            "Epoch 00195: loss improved from 1.55950 to 1.54095, saving model to model_weights_saved.hdf5\n",
            "Epoch 196/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.5164\n",
            "\n",
            "Epoch 00196: loss improved from 1.54095 to 1.51639, saving model to model_weights_saved.hdf5\n",
            "Epoch 197/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.4975\n",
            "\n",
            "Epoch 00197: loss improved from 1.51639 to 1.49747, saving model to model_weights_saved.hdf5\n",
            "Epoch 198/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.4799\n",
            "\n",
            "Epoch 00198: loss improved from 1.49747 to 1.47992, saving model to model_weights_saved.hdf5\n",
            "Epoch 199/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 1.4605\n",
            "\n",
            "Epoch 00199: loss improved from 1.47992 to 1.46052, saving model to model_weights_saved.hdf5\n",
            "Epoch 200/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.4346\n",
            "\n",
            "Epoch 00200: loss improved from 1.46052 to 1.43461, saving model to model_weights_saved.hdf5\n",
            "Epoch 201/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.4162\n",
            "\n",
            "Epoch 00201: loss improved from 1.43461 to 1.41623, saving model to model_weights_saved.hdf5\n",
            "Epoch 202/205\n",
            "9/9 [==============================] - 27s 3s/step - loss: 1.4147\n",
            "\n",
            "Epoch 00202: loss improved from 1.41623 to 1.41470, saving model to model_weights_saved.hdf5\n",
            "Epoch 203/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.3798\n",
            "\n",
            "Epoch 00203: loss improved from 1.41470 to 1.37981, saving model to model_weights_saved.hdf5\n",
            "Epoch 204/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.3478\n",
            "\n",
            "Epoch 00204: loss improved from 1.37981 to 1.34781, saving model to model_weights_saved.hdf5\n",
            "Epoch 205/205\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.3375\n",
            "\n",
            "Epoch 00205: loss improved from 1.34781 to 1.33754, saving model to model_weights_saved.hdf5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f73327dd190>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrs4PFDZPEtH"
      },
      "source": [
        "filename = \"model_weights_saved.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Asdd0dD_PEwv"
      },
      "source": [
        "num_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJsLfWd0QAw8",
        "outputId": "97b4ddfc-6f33-441f-e6a1-e676e0ef6f65"
      },
      "source": [
        "start = numpy.random.randint(0, len(x_data) - 1)\n",
        "pattern = x_data[start]\n",
        "print(\"Random Seed:\")\n",
        "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed:\n",
            "\" ng information readable state unintelligible nonsense sender encrypted message shares decoding techn \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "KMS4LzaiQA3P",
        "outputId": "d72c80da-e2a2-4c7e-952a-7b267dbd1e44"
      },
      "source": [
        "# generate the text\n",
        "for i in range(1000):\n",
        "  x = numpy.reshape(pattern, (1,len(pattern), 1))\n",
        "  x = x/float(vocab_len)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = num_to_char[index]\n",
        "  seq_in = [num_to_char[value] for value in pattern]\n",
        "  sys.stdout.write(result)\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[ 1:len(pattern)]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rtication aomputing pooor iovertication aomputing pooor iovertication aomputing pooor iovertication aomputing pooor iovertication aomputing pooor iovertication aomputing pooor iovertication aomputing pooor iovertication aomputing pooor iovertication aomputing pooor iovertication aomputing pooor iovertication aomputing pooor iovertication aomputing pooor iovertication aomputing pooor iovert"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-51fa7be13319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1728\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1901\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1902\u001b[0m     \"\"\"\n\u001b[0;32m-> 1903\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m   def interleave(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   5061\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5062\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 5063\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   5064\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5065\u001b[0m       raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   4216\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4218\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4219\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4220\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3149\u001b[0m     \"\"\"\n\u001b[1;32m   3150\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3151\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3152\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3153\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3114\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3115\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3116\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3117\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         if x is not None)\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m     \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;31m# Ensure all ops which must run do run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops_which_must_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops_which_must_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0mcontrol_output_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     for idx, r in enumerate(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}